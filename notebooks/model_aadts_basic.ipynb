{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime   \n",
    "from dateutil import relativedelta\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from shapely.wkt import loads\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsRegressor\n",
    "from sklearn.neighbors.regression import check_array, _get_weights\n",
    "\n",
    "def get_matching_s3_keys(bucket, prefix='', suffix='', contain=''):\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "\n",
    "    :param bucket: Name of the S3 bucket.\n",
    "    :param prefix: Only fetch keys that start with this prefix (optional).\n",
    "    :param suffix: Only fetch keys that end with this suffix (optional).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    kwargs = {'Bucket': bucket}\n",
    "\n",
    "    # If the prefix is a single string (not a tuple of strings), we can\n",
    "    # do the filtering directly in the S3 API.\n",
    "    if isinstance(prefix, str):\n",
    "        kwargs['Prefix'] = prefix\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # The S3 API response is a large blob of metadata.\n",
    "        # 'Contents' contains information about the listed objects.\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp['Contents']:\n",
    "            key = obj['Key']\n",
    "            if key.startswith(prefix) and key.endswith(suffix) and (contain in key):\n",
    "                yield key\n",
    "\n",
    "        # The S3 API is paginated, returning up to 1000 keys at a time.\n",
    "        # Pass the continuation token into the next response, until we\n",
    "        # reach the final page (when this field is missing).\n",
    "        try:\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "\n",
    "class MedianKNNRegressor(KNeighborsRegressor):\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "\n",
    "        neigh_dist, neigh_ind = self.kneighbors(X)\n",
    "\n",
    "        weights = _get_weights(neigh_dist, self.weights)\n",
    "\n",
    "        _y = self._y\n",
    "        \n",
    "        if _y.ndim == 1:\n",
    "            _y = _y.reshape((-1, 1))\n",
    "\n",
    "        ######## Begin modification\n",
    "        if weights is None:\n",
    "            y_pred = np.median(_y[neigh_ind], axis=1)\n",
    "        else:\n",
    "            # y_pred = weighted_median(_y[neigh_ind], weights, axis=1)\n",
    "            raise NotImplementedError(\"weighted median\")\n",
    "        ######### End modification\n",
    "\n",
    "        if self._y.ndim == 1:\n",
    "            y_pred = y_pred.ravel()\n",
    "\n",
    "        return y_pred    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already completed: shapes_development/patch_osm_ordered_v2/state=al/part-00113-3b31d96d-445d-4d0c-84b5-47f1f7f29c08.c000.csv\n"
     ]
    }
   ],
   "source": [
    "fd = 'shapes_development/patch_osm_ordered_v2/state=al/part-00113-3b31d96d-445d-4d0c-84b5-47f1f7f29c08.c000.csv'\n",
    "\n",
    "ofile = fd.replace('patch_osm_ordered_delivery_v2', 'patch_osm_ordered_v2')\n",
    "\n",
    "try:\n",
    "    for p in get_matching_s3_keys('inrixprod-volumes',\n",
    "                                  prefix=ofile):\n",
    "        print( \"Already completed: {}\".format(ofile) )\n",
    "except KeyError:\n",
    "    print('not completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes_development/patch_aadt_complete/state=al/\n",
      "shapes_development/patch_aadt_complete/state=ar/\n",
      "shapes_development/patch_aadt_complete/state=az/\n",
      "shapes_development/patch_aadt_complete/state=ca/\n",
      "shapes_development/patch_aadt_complete/state=co/\n",
      "shapes_development/patch_aadt_complete/state=ct/\n",
      "shapes_development/patch_aadt_complete/state=de/\n",
      "shapes_development/patch_aadt_complete/state=fl/\n",
      "shapes_development/patch_aadt_complete/state=ga/\n",
      "shapes_development/patch_aadt_complete/state=ia/\n",
      "shapes_development/patch_aadt_complete/state=id/\n",
      "shapes_development/patch_aadt_complete/state=il/\n",
      "shapes_development/patch_aadt_complete/state=in/\n",
      "shapes_development/patch_aadt_complete/state=ks/\n",
      "shapes_development/patch_aadt_complete/state=ky/\n",
      "shapes_development/patch_aadt_complete/state=la/\n",
      "shapes_development/patch_aadt_complete/state=ma/\n",
      "shapes_development/patch_aadt_complete/state=md/\n",
      "shapes_development/patch_aadt_complete/state=me/\n",
      "shapes_development/patch_aadt_complete/state=mi/\n",
      "shapes_development/patch_aadt_complete/state=mn/\n",
      "shapes_development/patch_aadt_complete/state=ms/\n",
      "shapes_development/patch_aadt_complete/state=mt/\n",
      "shapes_development/patch_aadt_complete/state=ny/\n",
      "shapes_development/patch_aadt_complete/state=tx/\n",
      "shapes_development/patch_aadt_complete/state=wa/\n"
     ]
    }
   ],
   "source": [
    "src = 'data_resources_temp'\n",
    "dest = 'outdata_temp'\n",
    "model = True\n",
    "frc_specific_ratios = True\n",
    "num = 'cross_counts'\n",
    "ignore_list = []\n",
    "\n",
    "for fg in get_matching_s3_keys('inrixprod-volumes',\n",
    "                               prefix= 'shapes_development/patch_aadt_complete/',\n",
    "                               suffix='.parquet.snappy'):\n",
    "    ignore_list.append(fg)\n",
    "\n",
    "for fg in get_matching_s3_keys('inrixprod-volumes',\n",
    "                               prefix='shapes_development/patch_aadt_by_state/',\n",
    "                               suffix='.csv'):\n",
    "\n",
    "    dest_s3 = fg.replace('shapes_development/patch_aadt_by_state/', 'shapes_development/patch_aadt_complete/')[:48]\n",
    "\n",
    "    if dest_s3 in ignore_list:\n",
    "        continue\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    if not os.path.exists(src):\n",
    "        os.mkdir(src)\n",
    "    if not os.path.exists(dest):\n",
    "        os.mkdir(dest)\n",
    "\n",
    "    pth_trip = os.path.join(src, 'trip_paths_byminute.csv')\n",
    "\n",
    "    s3.download_file('inrixprod-volumes',\n",
    "                     fg,\n",
    "                     pth_trip)\n",
    "\n",
    "    df_trip = pd.read_csv(pth_trip, escapechar='\\\\')\n",
    "    mxc = df_trip['minute_counts'].max()\n",
    "\n",
    "    df_trip['hpms_volume'] = df_trip['hpms_volume'] * 1.0 / 7\n",
    "\n",
    "    df_trip['crossing_norm'] = (df_trip['cross_counts'] - df_trip['cross_counts'].mean()) / df_trip[\n",
    "        'cross_counts'].std()\n",
    "    df_trip['frc_norm'] = (df_trip['frc'] - df_trip['frc'].mean()) / df_trip['frc'].std()\n",
    "\n",
    "    df_trip['latitude'] = df_trip['geometry'].apply(lambda x: loads(x).centroid.y)\n",
    "    df_trip['longitude'] = df_trip['geometry'].apply(lambda x: loads(x).centroid.x)\n",
    "\n",
    "    df_trip['lat_norm'] = (df_trip['latitude'] - df_trip['latitude'].mean()) / df_trip['latitude'].std()\n",
    "    df_trip['lon_norm'] = (df_trip['longitude'] - df_trip['longitude'].mean()) / df_trip['longitude'].std()\n",
    "\n",
    "    df_miss = df_trip[df_trip['hpms_volume'].apply(lambda x: np.isnan(x))]\n",
    "    df_trip = df_trip[df_trip['hpms_volume'].apply(lambda x: ~np.isnan(x))]\n",
    "\n",
    "    df_trip['prob'] = df_trip['minute_counts'].apply(lambda x: -np.log(1 - np.sqrt(x * 1.0 / mxc)))\n",
    "    df_miss['prob'] = df_miss['minute_counts'].apply(lambda x: -np.log(1 - np.sqrt(x * 1.0 / mxc)))\n",
    "\n",
    "    df_trip = df_trip[df_trip['prob'].apply(lambda x: ~np.isinf(x))]\n",
    "\n",
    "    pred_list = ['latitude', 'longitude', 'lat_norm', 'lon_norm', 'frc_norm', 'crossing_norm']\n",
    "\n",
    "    df_trip['ratio'] = df_trip['cross_counts'] / df_trip['hpms_volume']\n",
    "    df_miss['ratio'] = df_miss['cross_counts'] / df_miss['hpms_volume']\n",
    "\n",
    "    trip_gr = df_trip[['ratio', 'frc']].groupby('frc').quantile([0.1, 0.9]).reset_index()\n",
    "    trip_gr = trip_gr.pivot(index='frc', columns='level_1', values='ratio').reset_index()\n",
    "    trip_gr.columns = ['frc', 'mins', 'maxs']\n",
    "\n",
    "    df_trip = df_trip.merge(trip_gr, on='frc')\n",
    "\n",
    "    df_trip = df_trip[\n",
    "        pred_list + ['segid', 'roadname', 'frc', 'geometry', 'hpms_volume', 'cross_counts', 'minute_counts', 'prob',\n",
    "                     'ratio', 'mins', 'maxs']]\n",
    "    df_miss = df_miss[\n",
    "        pred_list + ['segid', 'roadname', 'frc', 'geometry', 'hpms_volume', 'cross_counts', 'minute_counts', 'prob',\n",
    "                     'ratio']]\n",
    "\n",
    "    df_train = df_trip[(df_trip['ratio'] > 0.01) & \\\n",
    "                       (df_trip['ratio'] < 10.0) & \\\n",
    "                       (df_trip['ratio'].apply(lambda x: ~np.isnan(x)))].copy()\n",
    "\n",
    "    df_test = df_trip.copy()\n",
    "    df_misst = df_miss.copy()\n",
    "\n",
    "    mod = MedianKNNRegressor(n_neighbors=300, n_jobs=8)\n",
    "    pred = ['lat_norm', 'lon_norm', 'crossing_norm']\n",
    "\n",
    "    mod.fit(df_train[pred], df_train['ratio'])\n",
    "\n",
    "    df_test['ratio_prd'] = mod.predict(df_test[pred])\n",
    "    df_misst['ratio_prd'] = mod.predict(df_misst[pred])\n",
    "\n",
    "    df_test['l360v'] = df_test['cross_counts'] / df_test['ratio_prd']\n",
    "    df_misst['l360v'] = df_misst['cross_counts'] / df_misst['ratio_prd']\n",
    "\n",
    "    df_test['raw_error'] = df_test['hpms_volume'] - df_test['l360v']\n",
    "    df_test['abs_error'] = df_test['raw_error'].apply(np.abs)\n",
    "    df_test['perc_error'] = df_test['raw_error'] / ((df_test['hpms_volume'] + df_test['l360v']) * 0.5)\n",
    "    df_test['perc_abs_error'] = df_test['perc_error'].apply(np.abs)\n",
    "\n",
    "    df_misst['raw_error'] = -1\n",
    "    df_misst['abs_error'] = -1\n",
    "    df_misst['perc_error'] = -1\n",
    "    df_misst['perc_abs_error'] = -1\n",
    "\n",
    "    cls = ['segid', 'roadname', 'frc', 'geometry', 'hpms_volume', 'l360v', 'cross_counts', 'raw_error', 'abs_error',\n",
    "           'perc_error', 'perc_abs_error', 'ratio_prd']\n",
    "\n",
    "    df_test = df_test[cls]\n",
    "    df_misst = df_misst[cls]\n",
    "\n",
    "    df_misst.columns = ['segid', 'roadname', 'frc', 'geometry', 'hpms_volume', 'predicted_volume', 'cross_counts',\n",
    "                        'raw_error', 'abs_error', 'perc_error', 'perc_abs_error', 'ratio']\n",
    "\n",
    "    df_test.columns = ['segid', 'roadname', 'frc', 'geometry', 'hpms_volume', 'predicted_volume', 'cross_counts',\n",
    "                       'raw_error', 'abs_error', 'perc_error', 'perc_abs_error', 'ratio']\n",
    "\n",
    "    df_test.to_parquet(os.path.join(dest, 'simple_observe.parquet.snappy'), index=False)\n",
    "\n",
    "    df_misst.to_parquet(os.path.join(dest, 'simple_predict.parquet.snappy'), index=False)\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    s3.upload_file(os.path.join(dest, 'simple_observe.parquet.snappy'),\n",
    "                   'inrixprod-volumes',\n",
    "                   dest_s3 + 'simple_observe.parquet.snappy')\n",
    "\n",
    "    s3.upload_file(os.path.join(dest, 'simple_predict.parquet.snappy'),\n",
    "                   'inrixprod-volumes',\n",
    "                   dest_s3 + 'simple_predict.parquet.snappy')\n",
    "\n",
    "    print('{} Complete'.format(dest_s3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
